{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import _pickle as cPickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense,Concatenate, Flatten, Lambda,Dropout,Layer, SpatialDropout1D, RepeatVector, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "#modify seed to verify results with different random seeds\n",
    "tf.random.set_seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from appropriate folders\n",
    "\n",
    "with open('train.txt','rb')as f:\n",
    "    train=cPickle.load(f)\n",
    "with open('test.txt','rb')as f:\n",
    "    test=cPickle.load(f)\n",
    "\n",
    "print(\"len of train\",len(train[0]))\n",
    "print(\"len of test\",len(test[0]))\n",
    "\n",
    "#train sequences and labels\n",
    "\n",
    "train_seq=train[0]\n",
    "train_y=train[1]\n",
    "\n",
    "#test sequences and labels\n",
    "\n",
    "test_seq=test[0]\n",
    "test_y=test[1]\n",
    "\n",
    "#vocab\n",
    "vocab = np.load(\"vocab_yoochoose_64.npy\")\n",
    "print(len(vocab))\n",
    "\n",
    "#embedding\n",
    "with open(\"transe_emb_new\",'rb')as f:\n",
    "    transe_emb=cPickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take recent 10 percent data as validation\n",
    "\n",
    "k = int(0.1*len(train_seq))\n",
    "\n",
    "val_y=train_y[-k:]\n",
    "train_y = train_y[:-k]\n",
    "val_x=train_seq[-k:]\n",
    "train_seq = train_seq[:-k]\n",
    "\n",
    "\n",
    "print(len(train_seq), len(val_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-order training data based on different Curriculums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the cell with required curriculum and comment the other cells (CL1 or CL2 or Hybrid or Reverse)\n",
    "\n",
    "#CL with last item (CL1)\n",
    "def find_similarity(a,b):\n",
    "    a_val= transe_emb[str(a)]\n",
    "    b_val= transe_emb[str(b)]\n",
    "    return dot(a_val, b_val)/(norm(a_val)*norm(b_val))\n",
    "\n",
    "new_train_seq=[]\n",
    "for i in range (0,len(train_seq)):\n",
    "    new_sub_seq =train_seq[i]\n",
    "    sim= find_similarity(train_seq[i][-1], train_y[i])\n",
    "#     print(train_seq[i][-1], train_y[i])\n",
    "    new_sub_seq.append(train_y[i])\n",
    "    new_sub_seq.append(sim)\n",
    "    new_train_seq.append(new_sub_seq)\n",
    "\n",
    "new_train_seq.sort(key=lambda x: (x[-1],len(x)), reverse=True)\n",
    "\n",
    "#reverse CL (uncomment the following line for reverse CL)\n",
    "# new_train_seq.sort(key=lambda x: (x[-1],len(x)), reverse=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CL with avg of last k items (k=10 got best results)  (CL2)\n",
    "def find_similarity(a,b):\n",
    "    a_emb=[]\n",
    "    for each in a:\n",
    "        a_emb.append(transe_emb[str(each)])\n",
    "    a_emb= np.array(a_emb)\n",
    "    a_val = np.mean(a_emb, axis=0)\n",
    "    b_val=transe_emb[str(b)]\n",
    "    return dot(a_val, b_val)/(norm(a_val)*norm(b_val))\n",
    "\n",
    "new_train_seq=[]\n",
    "for i in range (0,len(train_seq)):\n",
    "    new_sub_seq =train_seq[i]\n",
    "    sim= find_similarity(train_seq[i][-10:], train_y[i])\n",
    "    new_sub_seq.append(train_y[i])\n",
    "    new_sub_seq.append(sim)\n",
    "    new_train_seq.append(new_sub_seq)\n",
    "\n",
    "new_train_seq.sort(key=lambda x: (x[-1],len(x)), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CL with combination of both (Hybrid)\n",
    "def find_similarity1(a,b):\n",
    "    a_emb=[]\n",
    "    for each in a:\n",
    "        a_emb.append(transe_emb[str(each)])\n",
    "    a_emb= np.array(a_emb)\n",
    "    a_val = np.mean(a_emb, axis=0)\n",
    "    b_val=transe_emb[str(b)]\n",
    "    return dot(a_val, b_val)/(norm(a_val)*norm(b_val))\n",
    "\n",
    "def find_similarity2(a,b):\n",
    "    a_val= transe_emb[str(a)]\n",
    "    b_val= transe_emb[str(b)]\n",
    "    return dot(a_val, b_val)/(norm(a_val)*norm(b_val))\n",
    "\n",
    "new_train_seq=[]\n",
    "for i in range (0,len(train_seq)):\n",
    "    new_sub_seq =train_seq[i]\n",
    "    sim1= find_similarity1(train_seq[i][-10:], train_y[i])\n",
    "    sim2= find_similarity2(train_seq[i][-1], train_y[i])\n",
    "    sim=0.5*sim1+0.5*sim2\n",
    "    new_sub_seq.append(train_y[i])\n",
    "    new_sub_seq.append(sim)\n",
    "    new_train_seq.append(new_sub_seq)\n",
    "\n",
    "\n",
    "new_train_seq.sort(key=lambda x: (x[-1],len(x)), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new train y to be computed after any of CL approaches \n",
    "new_train_y=[]\n",
    "for i in range (0,len(new_train_seq)):\n",
    "    sim=new_train_seq[i].pop()\n",
    "    last_item=new_train_seq[i].pop()\n",
    "    new_train_y.append(last_item)\n",
    "\n",
    "def padding_seq(dictList):\n",
    "    for i in range(0,len(dictList)):\n",
    "        if len(dictList[i])>10:\n",
    "            max_len=len(dictList[i])\n",
    "            dictList[i]=dictList[i][-10:]\n",
    "        if len(dictList[i])<10:\n",
    "            w=10-len(dictList[i])\n",
    "            dictList[i]=['padding_id']*w+dictList[i]\n",
    "        \n",
    "    return dictList\n",
    "\n",
    "train_x=padding_seq(new_train_seq)\n",
    "test_x=padding_seq(test_seq)\n",
    "val_x=padding_seq(val_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to Suitable Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data\n",
    "#create a dictionary\n",
    "convert_dict=dict()\n",
    "convert_dict[\"padding_id\"]=0\n",
    "i=1\n",
    "for each in vocab:\n",
    "    convert_dict[each]=i\n",
    "    i=i+1\n",
    "\n",
    "#create embedding matrix\n",
    "embed_dim =200\n",
    "total_vocab = len(vocab)+1\n",
    "unknown_id=np.zeros((200,))\n",
    "vocab = list(vocab)\n",
    "embedding_matrix = np.zeros((total_vocab, embed_dim))\n",
    "embedding_matrix[0] = unknown_id\n",
    "\n",
    "for each in vocab:\n",
    "    embedding_matrix[convert_dict[each]] =transe_emb[str(each)]\n",
    "\n",
    "def type_conv(data):\n",
    "    data_new=[]\n",
    "    for each in data:\n",
    "        for i in range(0,len(each)):\n",
    "            try:\n",
    "                each[i]= convert_dict[str(each[i])]\n",
    "            except:\n",
    "                each[i]=0\n",
    "        data_new.append(each)\n",
    "    return data_new\n",
    "\n",
    "train_x= type_conv(train_x)\n",
    "val_x= type_conv(val_x)\n",
    "test_x= type_conv(test_x)\n",
    "    \n",
    "train_x=np.array(train_x)\n",
    "val_x=np.array(val_x)\n",
    "test_x=np.array(test_x)\n",
    "\n",
    "train_x = train_x.astype(int)\n",
    "val_x = val_x.astype(int)\n",
    "test_x = test_x.astype(int)\n",
    "\n",
    "train_y= [convert_dict[str(each)] for each in new_train_y]\n",
    "val_y= [convert_dict[str(each)] for each in val_y]\n",
    "test_y1=[]\n",
    "for each in test_y:\n",
    "    try:\n",
    "        test_y1.append(convert_dict[str(each)])\n",
    "    except:\n",
    "        test_y1.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM + CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSTM\n",
    "\n",
    "unknown_id=np.zeros((200,))\n",
    "def w2v_data_extraction(new_list):\n",
    "    w2v_data=[]\n",
    "    for i in range(0,len(new_list)):\n",
    "        seq_vec=[]\n",
    "        for j in range(0,len(new_list[i])):\n",
    "            try:\n",
    "                embedding= embedding_matrix[new_list[i][j]]\n",
    "#                 print(Embedding)\n",
    "            except (KeyError,ValueError, IndexError):\n",
    "                embedding=unknown_id\n",
    "            seq_vec.append(embedding)\n",
    "\n",
    "        w2v_data.append(seq_vec)\n",
    "    return np.asarray(w2v_data)\n",
    "\n",
    "#represent output as one-hot encoded\n",
    "def one_hot(seq,total_vocab):\n",
    "    seq_one_hot=np.zeros([len(seq),total_vocab])\n",
    "    for i in range(0,len(seq)):\n",
    "        seq_one_hot[i][seq[i]]=1\n",
    "    return seq_one_hot\n",
    "\n",
    "def generator(X_data, y_data, batch_size, total_vocab):\n",
    "    #shuffle data\n",
    "    indices = np.arange(X_data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_data= np.array(X_data)\n",
    "    y_data=np.array(y_data)\n",
    "    X_data = X_data[indices]\n",
    "    y_data = y_data[indices]\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    while 1:\n",
    "        X_batch = np.array(X_data[batch_size*counter:batch_size*(counter+1)])\n",
    "        X_batch = w2v_data_extraction(X_batch)\n",
    "        y_batch = np.array(y_data[batch_size*counter:batch_size*(counter+1)])\n",
    "        y_batch=one_hot(y_batch,total_vocab)\n",
    "        counter += 1\n",
    "        yield X_batch,y_batch\n",
    "\n",
    "        #restart counter to yeild data in the next epoch as well\n",
    "        if counter >= number_of_batches:\n",
    "            counter = 0\n",
    "\n",
    "total_vocab = len(vocab)+1\n",
    "batch_size=512\n",
    "\n",
    "\n",
    "\n",
    "top20accuracy = tf.keras.metrics.TopKCategoricalAccuracy(k=20)\n",
    "early_callback = tf.keras.callbacks.EarlyStopping(monitor='val_top_k_categorical_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "main_input = Input(shape=(20,200), name='main_input')\n",
    "lstm_out = LSTM(100,return_sequences=True)(main_input)\n",
    "lstm_out = Dropout(0.2)(lstm_out)\n",
    "print(lstm_out.shape)\n",
    "att = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                       bias_regularizer=tf.keras.regularizers.l1(1e-4),attention_regularizer_weight=1e-4,name='Attention')(lstm_out)\n",
    "att1 = Lambda(lambda xin: K.mean(xin, axis=-2), output_shape=(100,))(att)\n",
    "\n",
    "main_output = (Dense(total_vocab, activation='softmax', name='main_output')(att1))\n",
    "model = Model(inputs=main_input, outputs=main_output)\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy',top20accuracy],optimizer=\"adam\")\n",
    "\n",
    "\n",
    "#fit model (LSTM+CL)\n",
    "epochs=10\n",
    "i=0.1\n",
    "for j in range(epochs):\n",
    "    train_x_sample = train_x[0:int(i*len(train_x))]\n",
    "    train_y_sample = train_y[0:int(i*len(train_x))]\n",
    "        \n",
    "    if j==9:\n",
    "        model.fit_generator(generator(train_x_sample,train_y_sample,batch_size, total_vocab),epochs=10,\n",
    "                            steps_per_epoch = train_x_sample.shape[0]/batch_size,\n",
    "    validation_data = generator(test_x,test_y1,batch_size,total_vocab),\n",
    "    validation_steps = test_x.shape[0]/batch_size, callbacks=[early_callback])\n",
    "    \n",
    "    else:\n",
    "        model.fit_generator(generator(train_x_sample,train_y_sample,batch_size, total_vocab),epochs=1,\n",
    "                            steps_per_epoch = train_x_sample.shape[0]/batch_size,validation_data = generator(test_x,test_y1,batch_size,total_vocab),\n",
    "    validation_steps = test_x.shape[0]/batch_size, callbacks=[early_callback])\n",
    "          \n",
    "    if i< 1:\n",
    "        i=i+0.1\n",
    "print(\"model building done\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "def mrr_20(pred,actual):\n",
    "    predics = []\n",
    "\n",
    "    sum_of_inv_rank = 0\n",
    "\n",
    "    for i in range(0, len(pred)):\n",
    "        predics.append(np.argsort(pred[i])[-20:])\n",
    "    count = 0\n",
    "    for i in range(0, len(predics)):\n",
    "        # if actual[i] in predics[i]:\n",
    "        try:\n",
    "            rank = np.where(predics[i]==actual[i])[0][0] #predics[i].index(actual[i])\n",
    "            sum_of_inv_rank += 1/ (20-rank)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    return sum_of_inv_rank/len(actual)\n",
    "\n",
    "# Hit rate at 1 on test data\n",
    "def hit_rate_at_1(pred,actual):\n",
    "    # return accuracy_score(prediction,actual)\n",
    "    predics = []\n",
    "    for i in range(0, len(pred)):\n",
    "        predics.append(np.argsort(pred[i])[-1:])\n",
    "    count = 0\n",
    "    for i in range(0, len(predics)):\n",
    "        if actual[i] in predics[i]:\n",
    "            count = count + 1\n",
    "\n",
    "    return count/len(actual)\n",
    "\n",
    "# Hit rata at 5 on test data\n",
    "def hit_rate_at_20(pred,actual):\n",
    "    predics = []\n",
    "    for i in range(0, len(pred)):\n",
    "        predics.append(np.argsort(pred[i])[-20:])\n",
    "    count = 0\n",
    "    for i in range(0, len(predics)):\n",
    "        if actual[i] in predics[i]:\n",
    "            count = count + 1\n",
    "\n",
    "    return count/len(actual)\n",
    "\n",
    "# Hit rate at 10 on test data\n",
    "def hit_rate_at_10(pred, actual):\n",
    "    predics = []\n",
    "    for i in range(0, len(pred)):\n",
    "        predics.append(np.argsort(pred[i])[-10:])\n",
    "    count = 0\n",
    "    for i in range(0, len(predics)):\n",
    "        if actual[i] in predics[i]:\n",
    "            count = count + 1\n",
    "\n",
    "    return count /len(actual)\n",
    "\n",
    "# Prediction on test data for LSTM\n",
    "def model_predict(model,test_x,test_seq):\n",
    "    pred=model.predict(x=w2v_data_extraction(test_x))\n",
    "    preddy=np.argmax(a=pred,axis=1)\n",
    "\n",
    "\n",
    "    print(hit_rate_at_1(pred,test_seq))\n",
    "    print(hit_rate_at_10(pred, test_seq))\n",
    "    print(hit_rate_at_20(pred, test_seq))\n",
    "    print(mrr_20(pred, test_seq))\n",
    "\n",
    "# Prediction on test data for trasformer\n",
    "def model_predict1(model,test_x,test_seq):\n",
    "    pred=model.predict(x=test_x)\n",
    "    preddy=np.argmax(a=pred,axis=1)\n",
    "\n",
    "\n",
    "    print(hit_rate_at_1(pred,test_seq))\n",
    "    print(hit_rate_at_10(pred, test_seq))\n",
    "    print(hit_rate_at_20(pred, test_seq))\n",
    "    print(mrr_20(pred, test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction using LSTM model\n",
    "model_predict(model,test_x,test_y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers + CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformers\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, kernel_initializer='lecun_normal',\n",
    "                                activation='gelu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n",
    "             layers.Dense(embed_dim, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-4)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-4)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.add = tf.keras.layers.Add()\n",
    "    \n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs, use_causal_mask = False)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1= self.add([inputs, attn_output])\n",
    "        out1 = self.layernorm1(out1)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2= self.add([out1, ffn_output])\n",
    "        return self.layernorm2(out2)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,mask_zero=True,)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(X_data, y_data, batch_size, total_vocab):\n",
    "    #shuffle data\n",
    "    indices = np.arange(X_data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_data= np.array(X_data)\n",
    "    y_data=np.array(y_data)\n",
    "    X_data = X_data[indices]\n",
    "    y_data = y_data[indices]\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    while 1:\n",
    "        X_batch = np.array(X_data[batch_size*counter:batch_size*(counter+1)])\n",
    "        y_batch = np.array(y_data[batch_size*counter:batch_size*(counter+1)])\n",
    "        y_batch=one_hot(y_batch,total_vocab)\n",
    "        counter += 1\n",
    "        yield X_batch,y_batch\n",
    "\n",
    "        #restart counter to yeild data in the next epoch as well\n",
    "        if counter >= number_of_batches:\n",
    "            counter = 0\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch<2:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.2)\n",
    "\n",
    "\n",
    "#represent output as one-hot encoded\n",
    "def one_hot(seq,total_vocab):\n",
    "    seq_one_hot=np.zeros([len(seq),total_vocab])\n",
    "    for i in range(0,len(seq)):\n",
    "        seq_one_hot[i][seq[i]]=1\n",
    "    return seq_one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 200  # Embedding size for each token\n",
    "num_heads = 8 # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen=10\n",
    "dropout=0.1\n",
    "batch_size = 2048\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, total_vocab, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "# x = transformer_block(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "last_item=x[::,-1]\n",
    "outputs = layers.Dense(total_vocab, activation=\"softmax\")(last_item)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20accuracy = tf.keras.metrics.TopKCategoricalAccuracy(k=20)\n",
    "early_callback = tf.keras.callbacks.EarlyStopping(monitor='val_top_k_categorical_accuracy', patience=3, restore_best_weights=True)\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy',top20accuracy],optimizer=\"adam\")\n",
    "epochs=10\n",
    "i=0.1\n",
    "for j in range(epochs):\n",
    "    train_x_sample = train_x[0:int(i*len(train_x))]\n",
    "    train_y_sample = train_y[0:int(i*len(train_x))]\n",
    "        \n",
    "    if j==9:\n",
    "        history1=model.fit_generator(generator(train_x_sample,train_y_sample,batch_size, total_vocab),epochs=10,\n",
    "                            steps_per_epoch = train_x_sample.shape[0]/batch_size,\n",
    "        validation_data = generator(val_x,val_y,batch_size,total_vocab),\n",
    "    validation_steps = val_x.shape[0]/batch_size, callbacks=[early_callback])\n",
    "    \n",
    "    else:\n",
    "        history = model.fit_generator(generator(train_x_sample,train_y_sample,batch_size, total_vocab),epochs=1,\n",
    "                            steps_per_epoch = train_x_sample.shape[0]/batch_size,validation_data = generator(val_x,val_y,batch_size,total_vocab),\n",
    "    validation_steps = val_x.shape[0]/batch_size, callbacks=[early_callback])\n",
    "          \n",
    "    if i< 1:\n",
    "        i=i+0.1\n",
    "print(\"model building done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict1(model,test_x,test_seq):\n",
    "    pred=model.predict(x=test_x)\n",
    "    preddy=np.argmax(a=pred,axis=1)\n",
    "\n",
    "\n",
    "    print(hit_rate_at_1(pred,test_seq))\n",
    "    print(hit_rate_at_10(pred, test_seq))\n",
    "    print(hit_rate_at_20(pred, test_seq))\n",
    "    print(mrr_20(pred, test_seq))\n",
    "\n",
    " \n",
    "def mrr_20(pred,actual):\n",
    "    predics = []\n",
    "\n",
    "    sum_of_inv_rank = 0\n",
    "\n",
    "    for i in range(0, len(pred)):\n",
    "        predics.append(np.argsort(pred[i])[-20:])\n",
    "    count = 0\n",
    "    for i in range(0, len(predics)):\n",
    "        # if actual[i] in predics[i]:\n",
    "        try:\n",
    "            rank = np.where(predics[i]==actual[i])[0][0] #predics[i].index(actual[i])\n",
    "            sum_of_inv_rank += 1/ (20-rank)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    return sum_of_inv_rank/len(actual)\n",
    "\n",
    "# Hit rate at 1 on test data\n",
    "def hit_rate_at_1(pred,actual):\n",
    "    # return accuracy_score(prediction,actual)\n",
    "    predics = []\n",
    "    for i in range(0, len(pred)):\n",
    "        predics.append(np.argsort(pred[i])[-1:])\n",
    "    count = 0\n",
    "    for i in range(0, len(predics)):\n",
    "        if actual[i] in predics[i]:\n",
    "            count = count + 1\n",
    "\n",
    "    return count/len(actual)\n",
    "\n",
    "# Hit rata at 5 on test data\n",
    "def hit_rate_at_20(pred,actual):\n",
    "    predics = []\n",
    "    for i in range(0, len(pred)):\n",
    "        predics.append(np.argsort(pred[i])[-20:])\n",
    "    count = 0\n",
    "    for i in range(0, len(predics)):\n",
    "        if actual[i] in predics[i]:\n",
    "            count = count + 1\n",
    "\n",
    "    return count/len(actual)\n",
    "\n",
    "# Hit rate at 10 on test data\n",
    "def hit_rate_at_10(pred, actual):\n",
    "    predics = []\n",
    "    for i in range(0, len(pred)):\n",
    "        predics.append(np.argsort(pred[i])[-10:])\n",
    "    count = 0\n",
    "    for i in range(0, len(predics)):\n",
    "        if actual[i] in predics[i]:\n",
    "            count = count + 1\n",
    "\n",
    "    return count /len(actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict1(model,test_x,test_y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjustable CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS based CL (Adjustable CL)\n",
    "\n",
    "\n",
    "def generator_sample_weights(X_data, y_data,weights, batch_size, total_vocab):\n",
    "    #shuffle data\n",
    "    indices = np.arange(X_data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_data = X_data[indices]\n",
    "    y_data = y_data[indices]\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    while 1:\n",
    "        X_batch = np.array(X_data[batch_size*counter:batch_size*(counter+1)])\n",
    "        y_batch = np.array(y_data[batch_size*counter:batch_size*(counter+1)])\n",
    "        y_batch=one_hot(y_batch,total_vocab)\n",
    "        weights_batch = np.array(weights[batch_size*counter:batch_size*(counter+1)])\n",
    "        counter += 1\n",
    "#         print(X_batch.shape, y_batch.shape, weights_batch.shape)\n",
    "        yield X_batch,y_batch, weights_batch\n",
    "\n",
    "        #restart counter to yeild data in the next epoch as well\n",
    "        if counter >= number_of_batches:\n",
    "            counter = 0\n",
    "\n",
    "batch_size=2048\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 2:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.5)\n",
    "\n",
    "early_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "#represent output as one-hot encoded\n",
    "def one_hot(seq,total_vocab):\n",
    "    seq_one_hot=np.zeros([len(seq),total_vocab])\n",
    "    for i in range(0,len(seq)):\n",
    "        seq_one_hot[i][seq[i]]=1\n",
    "    return seq_one_hot\n",
    "individual_loss =tf.keras.losses.CategoricalCrossentropy(reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=optimizer)\n",
    "weights = np.ones(len(train_x),)\n",
    "val_weights = np.ones(len(val_x),)\n",
    "epochs=20\n",
    "lr=0.001\n",
    "for i in range(epochs):\n",
    "    model.fit_generator(\n",
    "    generator_sample_weights(train_x,train_y,weights,batch_size, total_vocab),\n",
    "    epochs=1,\n",
    "    steps_per_epoch = train_x.shape[0]/batch_size,\n",
    "    validation_data = generator(val_x,val_y,batch_size,total_vocab),\n",
    "    validation_steps = val_x.shape[0]/batch_size)\n",
    "   \n",
    "    pred= model.predict(x=train_x, batch_size= len(train_x))\n",
    "    print(pred.shape)\n",
    "    ind_loss=[]\n",
    "    step=10000\n",
    "    j=0\n",
    "    while 1:\n",
    "        ind_loss.extend(individual_loss(pred[j:j+step], one_hot(train_y[j:j+step],total_vocab)).numpy())\n",
    "        j=j+step\n",
    "        if j > len(train_x):\n",
    "            break\n",
    "    print(len(ind_loss))\n",
    "    ind_loss = np.array(ind_loss)\n",
    "    weights = 1/ind_loss\n",
    "    \n",
    "model_predict(model,test_x,test_y1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sree",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
